	
	Title: Semantic Similarity with BERT
	Author: [Prakhar Gandhi]( https://github.com/praKhr)
	Description: Natural Language Inference by fine-tuning BERT model on SNLI Corpus.

Note :- Instead of comparing neural networks training with BERT model retrained into 2 steps, I use concept of transfer learning by first training as a convolutional neural nets and then unfreeze the layer of pretrained bert model to account for the high loss of training dataset.

## Why this dataset?
1.As this dataset contains labels given as similarity token as neutral, agreement or disagreement between two pair of sentences while generating in a pandas DataFrame, it is perfect for this job. 
2.There is no imbalance also in classes.
3.EDA can be done optionally by converting sentences into encodings generated by model using multivariate charts.
4.Using word cloud we can visualize the sentences split into set of unique words according to frequency of them.

	
	
## Introduction
	
	Semantic Similarity is the task of determining how similar
	two sentences are, in terms of what they mean.
	This example demonstrates the use of SNLI (Stanford Natural Language Inference) Corpus
	to predict sentence semantic similarity with Transformers.
	We will fine-tune a BERT model that takes two sentences as inputs
	and that outputs a similarity score for these two sentences.
	
### References
	
	* [BERT](https://arxiv.org/pdf/1810.04805.pdf)
	* [SNLI](https://nlp.stanford.edu/projects/snli/)
	
		
## Setup
	
	Note: install HuggingFace `transformers` via `pip install transformers` (version >= 2.11.0).
	
	import numpy as np
	import pandas as pd
	import tensorflow as tf
	import transformers
	

	
## Configuration
	
	

	max_length = 128  # Maximum length of input sentence to the model.
	batch_size = 32
	epochs = 2
	

# Labels in our dataset.
	labels = ["contradiction", "entailment", "neutral"]
	

	
## Load the Data


# There are more than 550k samples in total; we will use 100k for this example.

	

	
	Dataset Overview:
	
	- sentence1: The premise caption that was supplied to the author of the pair.
	- sentence2: The hypothesis caption that was written by the author of the pair.
	- similarity: This is the label chosen by the majority of annotators.
	Where no majority exists, the label "-" is used (we will skip such samples here).
	
	Here are the "similarity" label values in our dataset:
	
	- Contradiction: The sentences share no similarity.
	- Entailment: The sentences have similar meaning.
	- Neutral: The sentences are neutral.
	
	

## Preprocessing
	
	

	We have some NaN entries in our train data, we will simply drop them.

	We will further check the distribution of our training targets,

	And Distribution of our validation targets.


	The value "-" appears as part of our training and validation targets.
	We will skip these samples.

	
	One-hot encode training, validation, and test labels.
## Create a custom data generator
	

	class BertSemanticDataGenerator(tf.keras.utils.Sequence):
	    """Generates batches of data.
	
	    Args:
	        sentence_pairs: Array of premise and hypothesis input sentences.
	        labels: Array of labels.
	        batch_size: Integer batch size.
	        shuffle: boolean, whether to shuffle the data.
	        include_targets: boolean, whether to incude the labels.
	
	    Returns:
	        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`
	        (or just `[input_ids, attention_mask, `token_type_ids]`
	         if `include_targets=False`)
	    """
	
  ## Build the model
	
	# Create the model under a distribution strategy scope.
	strategy = tf.distribute.MirroredStrategy()
	

	with strategy.scope():
	    # Encoded token ids from BERT tokenizer.
	    input_ids = tf.keras.layers.Input(
	        shape=(max_length,), dtype=tf.int32, name="input_ids"
	    )
	    # Attention masks indicates to the model which tokens should be attended to.
	    attention_masks = tf.keras.layers.Input(
	        shape=(max_length,), dtype=tf.int32, name="attention_masks"
	    )
	    # Token type ids are binary masks identifying different sequences in the model.
	    token_type_ids = tf.keras.layers.Input(
	        shape=(max_length,), dtype=tf.int32, name="token_type_ids"
	    )
	    # Loading pretrained BERT model.
	    bert_model = transformers.TFBertModel.from_pretrained("bert-base-uncased")
	    # Freeze the BERT model to reuse the pretrained features without modifying them.
	    bert_model.trainable = False
	

	    bert_output = bert_model(
	        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids
	    )
	    sequence_output = bert_output.last_hidden_state
	    pooled_output = bert_output.pooler_output
	

	    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.
	    bi_lstm = tf.keras.layers.Bidirectional(
	        tf.keras.layers.LSTM(64, return_sequences=True)
	    )(sequence_output)
	    # Applying hybrid pooling approach to bi_lstm sequence output.
	    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)
	    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)
	    concat = tf.keras.layers.concatenate([avg_pool, max_pool])
	    dropout = tf.keras.layers.Dropout(0.3)(concat)
	    output = tf.keras.layers.Dense(3, activation="softmax")(dropout)
	    model = tf.keras.models.Model(
	        inputs=[input_ids, attention_masks, token_type_ids], outputs=output
	    )
	

	    model.compile(
	        optimizer=tf.keras.optimizers.Adam(),
	        loss="categorical_crossentropy",
	        metrics=["acc"],
	    )
	

	

	print(f"Strategy: {strategy}")
	model.summary()
	

	
	Create train and validation data generators

## Train the Model
	
	Training is done only for the top layers to perform "feature extraction",
	which will allow the model to use the representations of the pretrained model.


## Fine-tuning
	
	This step must only be performed after the feature extraction model has
	been trained to convergence on the new data.
	
	This is an optional last step where `bert_model` is unfreezed and retrained
	with a very low learning rate. This can deliver meaningful improvement by
	incrementally adapting the pretrained features to the new data.
	
	

	# Unfreeze the bert_model.
	bert_model.trainable = True
	# Recompile the model to make the change effective.
	model.compile(
	    optimizer=tf.keras.optimizers.Adam(1e-5),
	    loss="categorical_crossentropy",
	    metrics=["accuracy"],
	)


	
## Train the entire model end-to-end
	
	history = model.fit(
	    train_data,
	    validation_data=valid_data,
	    epochs=epochs,
	    use_multiprocessing=True,
	    workers=-1,
	)
	
## Evaluate model on the test set
	
	test_data = BertSemanticDataGenerator(
	    test_df[["sentence1", "sentence2"]].values.astype("str"),
	    y_test,
	    batch_size=batch_size,
	    shuffle=False,
	)
	model.evaluate(test_data, verbose=1)
	

	
## Inference on custom sentences is also done
	
	


